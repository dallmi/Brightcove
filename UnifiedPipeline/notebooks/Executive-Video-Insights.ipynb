{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Executive Video Insights -- Interview Preparation Reference\n",
    "\n",
    "**Purpose:** Comprehensive analysis of video performance across 11 Brightcove accounts  \n",
    "**Framework:** STAR-based talking points with quantified metrics for interview preparation  \n",
    "**Data Source:** Unified daily_analytics table in DuckDB (consolidated ETL pipeline output)\n",
    "\n",
    "---\n",
    "\n",
    "### How to Use This Notebook\n",
    "\n",
    "1. **Run all cells** -- each section computes metrics and stores them for the final cheat sheet\n",
    "2. **Review each \"Key Takeaway\"** -- fill in the **[bracketed placeholders]** with your actual numbers\n",
    "3. **Jump to Section 11** for the compiled interview cheat sheet and 7 STAR talking points\n",
    "4. **Customize** -- modify SQL queries to focus on specific time periods or channels\n",
    "\n",
    "### Sections\n",
    "\n",
    "| # | Section | Business Question |\n",
    "|---|---------|-------------------|\n",
    "| 1 | Executive Summary Dashboard | Overall platform health at a glance |\n",
    "| 2 | Viewing Volume & Adoption Trends | Is video consumption growing? |\n",
    "| 3 | Engagement Quality Scorecard | Are viewers actually watching our content? |\n",
    "| 4 | Engagement Funnel & Drop-off (The Big Win) | Where exactly do viewers drop off? |\n",
    "| 5 | Content Strategy: Duration Sweet Spot | What video length drives the best engagement? |\n",
    "| 6 | Top Performing Content & Content Gaps | Greatest hits vs. problem videos |\n",
    "| 7 | Channel/Account Performance Comparison | Where should we invest or consolidate? |\n",
    "| 8 | Device & Platform Strategy | How are employees watching videos? |\n",
    "| 9 | Content Lifecycle & Freshness | Which content is stale? |\n",
    "| 10 | Regional & Temporal Patterns | When and where are employees watching? |\n",
    "| 11 | Interview Cheat Sheet | All key numbers + 7 STAR talking points |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 0: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Imports & Configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "\n",
    "# Display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "pd.set_option('display.max_colwidth', 60)\n",
    "\n",
    "# Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "FIGSIZE = (12, 5)\n",
    "\n",
    "# Color scheme\n",
    "C = {\n",
    "    'success':  '#27ae60',\n",
    "    'fail':     '#e74c3c',\n",
    "    'warn':     '#f39c12',\n",
    "    'neutral':  '#2c3e50',\n",
    "    'gray':     '#95a5a6',\n",
    "    'success_light': '#a9dfbf',\n",
    "    'fail_light':    '#f5b7b1',\n",
    "    'warn_light':    '#fad7a0',\n",
    "    'neutral_light': '#aeb6bf',\n",
    "    'gray_light':    '#d5dbdb',\n",
    "    'blue':     '#2980b9',\n",
    "    'blue_light': '#85c1e9',\n",
    "    'purple':   '#8e44ad',\n",
    "    'purple_light': '#d2b4de',\n",
    "}\n",
    "\n",
    "# Funnel palette (gradient from green to red)\n",
    "FUNNEL_COLORS = [C['success'], C['blue'], C['warn'], '#e67e22', C['fail']]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Database connection\n",
    "# ---------------------------------------------------------------------------\n",
    "DB_PATH = Path('../output/analytics.duckdb')\n",
    "if not DB_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Database not found at {DB_PATH}. Run the pipeline first.\")\n",
    "\n",
    "conn = duckdb.connect(str(DB_PATH), read_only=True)\n",
    "\n",
    "\n",
    "def query(sql):\n",
    "    \"\"\"Execute SQL and return a DataFrame.\"\"\"\n",
    "    return conn.execute(sql).fetchdf()\n",
    "\n",
    "\n",
    "def fmt_num(n):\n",
    "    \"\"\"Format number with comma separators.\"\"\"\n",
    "    if pd.isna(n):\n",
    "        return 'N/A'\n",
    "    return f\"{int(n):,}\"\n",
    "\n",
    "\n",
    "def fmt_pct(p):\n",
    "    \"\"\"Format percentage to one decimal.\"\"\"\n",
    "    if pd.isna(p):\n",
    "        return 'N/A'\n",
    "    return f\"{p:.1f}%\"\n",
    "\n",
    "\n",
    "def truncate_name(name, max_len=50):\n",
    "    \"\"\"Truncate long video names for chart labels.\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return '(untitled)'\n",
    "    return name[:max_len] + '...' if len(str(name)) > max_len else str(name)\n",
    "\n",
    "\n",
    "def annotate_bars(ax, fmt=',.0f', suffix='', fontsize=9, offset=0.5):\n",
    "    \"\"\"Add value labels to bar chart patches.\"\"\"\n",
    "    for p in ax.patches:\n",
    "        val = p.get_width() if p.get_width() != 0 else p.get_height()\n",
    "        if p.get_width() > p.get_height():  # horizontal bar\n",
    "            ax.text(p.get_width() + offset, p.get_y() + p.get_height()/2,\n",
    "                    f'{val:{fmt}}{suffix}', va='center', fontsize=fontsize)\n",
    "        else:  # vertical bar\n",
    "            ax.text(p.get_x() + p.get_width()/2, p.get_height() + offset,\n",
    "                    f'{val:{fmt}}{suffix}', ha='center', fontsize=fontsize)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Metric accumulators for the final cheat sheet (Section 11)\n",
    "# ---------------------------------------------------------------------------\n",
    "EXEC = {}       # Executive summary KPIs\n",
    "TRENDS = {}     # Viewing trends\n",
    "ENGAGEMENT = {} # Engagement quality\n",
    "FUNNEL = {}     # Engagement funnel\n",
    "CONTENT = {}    # Duration / content strategy\n",
    "TOPVIDS = {}    # Top performing content\n",
    "CHANNELS = {}   # Channel performance\n",
    "DEVICE = {}     # Device strategy\n",
    "LIFECYCLE = {}  # Content lifecycle\n",
    "REGIONAL = {}   # Regional patterns\n",
    "\n",
    "print(f\"Connected to: {DB_PATH}\")\n",
    "print(f\"Database size: {DB_PATH.stat().st_size / (1024*1024):.1f} MB\")\n",
    "print(f\"Notebook generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Executive Summary Dashboard\n",
    "\n",
    "**Business Question:** What is the overall health of our video platform, at a glance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s1-kpis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1. Executive Summary -- KPIs\n",
    "# ---------------------------------------------------------------------------\n",
    "kpi = query(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(DISTINCT video_id)                              AS total_videos,\n",
    "        COUNT(DISTINCT channel)                               AS total_channels,\n",
    "        SUM(video_view)                                       AS total_views,\n",
    "        SUM(video_impression)                                 AS total_impressions,\n",
    "        ROUND(SUM(video_seconds_viewed) / 3600.0, 0)         AS total_watch_hours,\n",
    "        ROUND(AVG(engagement_score), 1)                       AS avg_engagement,\n",
    "        ROUND(AVG(video_engagement_100), 1)                   AS avg_completion_rate,\n",
    "        ROUND(SUM(video_view)*100.0 / NULLIF(SUM(video_impression),0), 1) AS play_rate,\n",
    "        MIN(date)                                             AS period_start,\n",
    "        MAX(date)                                             AS period_end,\n",
    "        DATEDIFF('day', MIN(date), MAX(date))                 AS period_days\n",
    "    FROM daily_analytics\n",
    "\"\"\").iloc[0]\n",
    "\n",
    "# Store for cheat sheet\n",
    "EXEC = kpi.to_dict()\n",
    "\n",
    "# Print dashboard\n",
    "print(\"=\" * 66)\n",
    "print(\"          EXECUTIVE SUMMARY -- VIDEO PLATFORM HEALTH\")\n",
    "print(\"=\" * 66)\n",
    "print(f\"\")\n",
    "print(f\"  Period:                {kpi['period_start']}  to  {kpi['period_end']}  ({int(kpi['period_days'])} days)\")\n",
    "print(f\"  Data freshness:        {(datetime.now().date() - pd.Timestamp(kpi['period_end']).date()).days} days since last data point\")\n",
    "print(f\"\")\n",
    "print(f\"  Total Videos:          {fmt_num(kpi['total_videos'])}\")\n",
    "print(f\"  Total Channels:        {fmt_num(kpi['total_channels'])}\")\n",
    "print(f\"  Total Views:           {fmt_num(kpi['total_views'])}\")\n",
    "print(f\"  Total Impressions:     {fmt_num(kpi['total_impressions'])}\")\n",
    "print(f\"  Total Watch Hours:     {fmt_num(kpi['total_watch_hours'])}\")\n",
    "print(f\"\")\n",
    "print(f\"  Avg Engagement Score:  {fmt_pct(kpi['avg_engagement'])}\")\n",
    "print(f\"  Avg Completion Rate:   {fmt_pct(kpi['avg_completion_rate'])}\")\n",
    "print(f\"  Play Rate:             {fmt_pct(kpi['play_rate'])}  (views / impressions)\")\n",
    "print(\"=\" * 66)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1-takeaway",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "\n",
    "> I built and managed a unified video analytics platform covering **[total_videos]** videos across **[total_channels]** Brightcove accounts in 4 business categories (internet/intranet, research, global wealth management, events). Over **[period_days]** days, the platform tracked **[total_views]** views and **[total_watch_hours]** watch hours, with an average engagement score of **[avg_engagement]** and a play rate of **[play_rate]**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Viewing Volume & Adoption Trends\n",
    "\n",
    "**Business Question:** Is video consumption growing? What are the trends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s2-trends",
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# 2. Viewing Volume & Adoption Trends\n# ---------------------------------------------------------------------------\n# Exclude the current (incomplete) month to avoid skewing trend comparisons\nmonthly = query(\"\"\"\n    SELECT\n        DATE_TRUNC('month', date)      AS month,\n        SUM(video_view)                AS total_views,\n        COUNT(DISTINCT video_id)       AS unique_videos\n    FROM daily_analytics\n    WHERE video_view > 0\n      AND DATE_TRUNC('month', date) < DATE_TRUNC('month', CURRENT_DATE)\n    GROUP BY 1\n    ORDER BY 1\n\"\"\")\n\n# Growth calculation: first complete month vs last complete month\nif len(monthly) >= 2:\n    first_views = monthly['total_views'].iloc[0]\n    last_views  = monthly['total_views'].iloc[-1]\n    growth_pct  = (last_views - first_views) / first_views * 100 if first_views > 0 else 0\n    avg_monthly = monthly['total_views'].mean()\n    trend_dir   = 'upward' if growth_pct > 5 else ('downward' if growth_pct < -5 else 'stable')\nelse:\n    growth_pct = 0\n    avg_monthly = monthly['total_views'].mean()\n    trend_dir = 'insufficient data'\n\nTRENDS = {\n    'growth_pct': growth_pct,\n    'avg_monthly_views': avg_monthly,\n    'trend_direction': trend_dir,\n    'num_months': len(monthly),\n    'first_month_views': first_views if len(monthly) >= 2 else 0,\n    'last_month_views': last_views if len(monthly) >= 2 else 0,\n}\n\n# -- Chart: Monthly views (bars) + unique videos (line overlay) --\nfig, ax1 = plt.subplots(figsize=FIGSIZE)\n\nx = range(len(monthly))\nlabels = monthly['month'].dt.strftime('%Y-%m').tolist()\n\nbars = ax1.bar(x, monthly['total_views'], color=C['blue_light'], edgecolor=C['blue'],\n               linewidth=0.5, label='Total Views', zorder=2)\nax1.set_ylabel('Total Views', fontsize=11, color=C['blue'])\nax1.yaxis.set_major_formatter(mticker.FuncFormatter(lambda v, _: fmt_num(v)))\nax1.tick_params(axis='y', labelcolor=C['blue'])\n\nax2 = ax1.twinx()\nax2.plot(x, monthly['unique_videos'], color=C['fail'], marker='o', linewidth=2,\n         markersize=5, label='Unique Videos Viewed', zorder=3)\nax2.set_ylabel('Unique Videos Viewed', fontsize=11, color=C['fail'])\nax2.tick_params(axis='y', labelcolor=C['fail'])\n\nax1.set_xticks(x)\nax1.set_xticklabels(labels, rotation=45, ha='right', fontsize=9)\nax1.set_title('Monthly Viewing Volume & Content Breadth (complete months only)',\n              fontsize=14, fontweight='bold')\n\n# Combined legend\nlines1, labels1 = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', framealpha=0.9)\n\nplt.tight_layout()\nplt.show()\n\n# Summary stats\nprint(f\"Growth (first complete month vs last complete month): {growth_pct:+.1f}%\")\nprint(f\"Average monthly views: {fmt_num(avg_monthly)}\")\nprint(f\"Trend direction: {trend_dir}\")"
  },
  {
   "cell_type": "markdown",
   "id": "s2-takeaway",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "\n",
    "> Over **[num_months]** months, video consumption showed a **[trend_direction]** trend with **[growth_pct]** growth from the first to the last measured period. Average monthly views reached **[avg_monthly_views]**, with content breadth tracked through unique videos viewed each month -- demonstrating the platform's value as a communication channel across the organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Engagement Quality Scorecard\n",
    "\n",
    "**Business Question:** Are viewers actually watching our content? Is quality improving?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s3-quality",
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# 3. Engagement Quality Scorecard\n# ---------------------------------------------------------------------------\n\n# Monthly rolling averages (exclude current incomplete month)\nquality_monthly = query(\"\"\"\n    SELECT\n        DATE_TRUNC('month', date)                       AS month,\n        ROUND(AVG(engagement_score), 1)                 AS avg_engagement,\n        ROUND(AVG(video_engagement_100), 1)             AS completion_rate,\n        ROUND(AVG(video_engagement_50), 1)              AS halfway_rate\n    FROM daily_analytics\n    WHERE video_view > 0\n      AND DATE_TRUNC('month', date) < DATE_TRUNC('month', CURRENT_DATE)\n    GROUP BY 1\n    ORDER BY 1\n\"\"\")\n\n# Overall averages for scorecard\noverall = query(\"\"\"\n    SELECT\n        ROUND(AVG(engagement_score), 1)                 AS avg_engagement,\n        ROUND(AVG(video_engagement_100), 1)             AS completion_rate,\n        ROUND(AVG(video_engagement_50), 1)              AS halfway_rate,\n        ROUND(AVG(video_percent_viewed), 1)             AS avg_percent_viewed\n    FROM daily_analytics\n    WHERE video_view > 0\n\"\"\").iloc[0]\n\nENGAGEMENT = overall.to_dict()\n\n# -- Chart: Monthly quality trends --\nfig, ax = plt.subplots(figsize=FIGSIZE)\n\nmonths_str = quality_monthly['month'].dt.strftime('%Y-%m').tolist()\nax.plot(months_str, quality_monthly['avg_engagement'], marker='o', linewidth=2,\n        color=C['blue'], label='Engagement Score')\nax.plot(months_str, quality_monthly['completion_rate'], marker='s', linewidth=2,\n        color=C['success'], label='Completion Rate (100%)')\nax.plot(months_str, quality_monthly['halfway_rate'], marker='^', linewidth=2,\n        color=C['warn'], label='Halfway Rate (50%)')\n\nax.set_ylim(0, 100)\nax.set_xlabel('Month', fontsize=11)\nax.set_ylabel('Percentage (%)', fontsize=11)\nax.set_title('Engagement Quality Trends Over Time (complete months only)',\n             fontsize=14, fontweight='bold')\nax.legend(loc='best', framealpha=0.9)\nax.tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n# -- Quality Assessment Table --\ndef rate(val, target_good, target_fair):\n    if val >= target_good:\n        return 'Good'\n    elif val >= target_fair:\n        return 'Fair'\n    return 'Needs Improvement'\n\nscorecard = pd.DataFrame([\n    {'Metric': 'Engagement Score', 'Value': fmt_pct(overall['avg_engagement']),\n     'Target': '>50%', 'Rating': rate(overall['avg_engagement'], 50, 35)},\n    {'Metric': 'Completion Rate',  'Value': fmt_pct(overall['completion_rate']),\n     'Target': '>40%', 'Rating': rate(overall['completion_rate'], 40, 25)},\n    {'Metric': 'Halfway Rate',     'Value': fmt_pct(overall['halfway_rate']),\n     'Target': '>55%', 'Rating': rate(overall['halfway_rate'], 55, 40)},\n    {'Metric': 'Avg % Viewed',     'Value': fmt_pct(overall['avg_percent_viewed']),\n     'Target': '>50%', 'Rating': rate(overall['avg_percent_viewed'], 50, 35)},\n])\n\nprint(\"\\nENGAGEMENT QUALITY SCORECARD\")\nprint(\"=\" * 66)\ndisplay(scorecard)\n\n# Trend direction\nif len(quality_monthly) >= 3:\n    first_eng = quality_monthly['avg_engagement'].iloc[:3].mean()\n    last_eng  = quality_monthly['avg_engagement'].iloc[-3:].mean()\n    eng_trend = 'improving' if last_eng > first_eng + 1 else ('declining' if last_eng < first_eng - 1 else 'stable')\n    ENGAGEMENT['trend'] = eng_trend\n    print(f\"\\nEngagement trend (first 3 months avg vs last 3 months avg): {eng_trend}\")\n    print(f\"  Early period: {first_eng:.1f}%  |  Recent period: {last_eng:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "id": "s3-takeaway",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "\n",
    "> Our engagement quality is rated **[rating]** overall: average engagement score of **[avg_engagement]**, completion rate of **[completion_rate]**, and halfway rate of **[halfway_rate]**. The trend is **[trend_direction]** over the measured period, suggesting **[improving content quality / opportunity for content optimization]**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Engagement Funnel & Drop-off Analysis (The Big Win)\n",
    "\n",
    "**Business Question:** Where exactly do viewers drop off, and what does that tell us about content quality?\n",
    "\n",
    "This is the **most actionable finding** -- the engagement funnel reveals precisely where content loses viewers, enabling targeted improvements to content format and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s4-funnel",
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# 4a. Overall Engagement Funnel\n# ---------------------------------------------------------------------------\n# The video_engagement_X fields are raw view counts at each percentile point.\n# Values can exceed video_view due to replays/rewinds (Brightcove counts every\n# pass through a percentile). Some accounts (e.g. Internet) have autoplay/loop\n# behavior that inflates these counts. We show two views:\n#   1. All channels (raw, may exceed 100% due to replays)\n#   2. Excluding outlier channels where replay inflation distorts the funnel\n\n# --- Overall funnel (all channels) ---\nfunnel_all = query(\"\"\"\n    SELECT\n        ROUND(SUM(video_engagement_1)   * 100.0 / NULLIF(SUM(video_view), 0), 1) AS started,\n        ROUND(SUM(video_engagement_25)  * 100.0 / NULLIF(SUM(video_view), 0), 1) AS reached_25,\n        ROUND(SUM(video_engagement_50)  * 100.0 / NULLIF(SUM(video_view), 0), 1) AS reached_50,\n        ROUND(SUM(video_engagement_75)  * 100.0 / NULLIF(SUM(video_view), 0), 1) AS reached_75,\n        ROUND(SUM(video_engagement_100) * 100.0 / NULLIF(SUM(video_view), 0), 1) AS completed\n    FROM daily_analytics\n    WHERE video_view > 0\n\"\"\").iloc[0]\n\n# Identify channels with replay inflation (started > 100%)\nreplay_channels = query(\"\"\"\n    SELECT channel,\n        ROUND(SUM(video_engagement_1) * 100.0 / NULLIF(SUM(video_view), 0), 1) AS started\n    FROM daily_analytics\n    WHERE video_view > 0\n    GROUP BY channel\n    HAVING SUM(video_engagement_1) * 100.0 / NULLIF(SUM(video_view), 0) > 100\n\"\"\")\nreplay_ch_list = replay_channels['channel'].tolist()\n\n# --- Funnel excluding replay-inflated channels ---\nif len(replay_ch_list) > 0:\n    placeholders = ', '.join(f\"'{c}'\" for c in replay_ch_list)\n    funnel = query(f\"\"\"\n        SELECT\n            ROUND(SUM(video_engagement_1)   * 100.0 / NULLIF(SUM(video_view), 0), 1) AS started,\n            ROUND(SUM(video_engagement_25)  * 100.0 / NULLIF(SUM(video_view), 0), 1) AS reached_25,\n            ROUND(SUM(video_engagement_50)  * 100.0 / NULLIF(SUM(video_view), 0), 1) AS reached_50,\n            ROUND(SUM(video_engagement_75)  * 100.0 / NULLIF(SUM(video_view), 0), 1) AS reached_75,\n            ROUND(SUM(video_engagement_100) * 100.0 / NULLIF(SUM(video_view), 0), 1) AS completed\n        FROM daily_analytics\n        WHERE video_view > 0\n          AND channel NOT IN ({placeholders})\n    \"\"\").iloc[0]\n    print(f\"NOTE: Excluding {replay_ch_list} from funnel (replay/autoplay inflation > 100%).\")\n    print(f\"These channels are analyzed separately in the per-channel table below.\\n\")\nelse:\n    funnel = funnel_all\n\nstages = ['Started (1%)', 'Reached 25%', 'Reached 50%', 'Reached 75%', 'Completed (100%)']\nvalues = [funnel['started'], funnel['reached_25'], funnel['reached_50'],\n          funnel['reached_75'], funnel['completed']]\n\n# Calculate drop-offs between stages\ndropoffs = {\n    '1% to 25%':   values[0] - values[1],\n    '25% to 50%':  values[1] - values[2],\n    '50% to 75%':  values[2] - values[3],\n    '75% to 100%': values[3] - values[4],\n}\nbiggest_drop_stage = max(dropoffs, key=dropoffs.get)\nbiggest_drop_val   = dropoffs[biggest_drop_stage]\n\nFUNNEL = {\n    'started': values[0], 'reached_25': values[1], 'reached_50': values[2],\n    'reached_75': values[3], 'completed': values[4],\n    'dropoffs': dropoffs,\n    'biggest_drop_stage': biggest_drop_stage,\n    'biggest_drop_val': biggest_drop_val,\n    'excluded_channels': replay_ch_list,\n}\n\n# -- Chart: Funnel --\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\ny_pos = range(len(stages) - 1, -1, -1)\nbars = ax1.barh(y_pos, values, color=FUNNEL_COLORS, edgecolor='white', linewidth=1.5, height=0.6)\nax1.set_yticks(y_pos)\nax1.set_yticklabels(stages[::-1], fontsize=11)\nax1.set_xlabel('Percentage of Viewers (%)', fontsize=11)\nax1.set_xlim(0, max(values) * 1.2)\nsubtitle = ' (excl. replay-inflated channels)' if replay_ch_list else ''\nax1.set_title(f'Engagement Funnel{subtitle}', fontsize=14, fontweight='bold')\n\nfor bar, val in zip(bars, values[::-1]):\n    ax1.text(val + 1, bar.get_y() + bar.get_height()/2,\n             fmt_pct(val), va='center', fontsize=11, fontweight='bold')\n\n# Right: drop-off between stages\ndrop_labels = list(dropoffs.keys())\ndrop_values = list(dropoffs.values())\ndrop_colors = [C['fail'] if v == biggest_drop_val else C['warn_light'] for v in drop_values]\n\ndrop_bars = ax2.barh(range(len(drop_labels) - 1, -1, -1), drop_values,\n                     color=drop_colors, edgecolor='white', linewidth=1.5, height=0.6)\nax2.set_yticks(range(len(drop_labels) - 1, -1, -1))\nax2.set_yticklabels(drop_labels[::-1], fontsize=11)\nax2.set_xlabel('Drop-off (percentage points)', fontsize=11)\nax2.set_title('Drop-off Between Stages', fontsize=14, fontweight='bold')\n\nfor bar, val in zip(drop_bars, drop_values[::-1]):\n    ax2.text(val + 0.3, bar.get_y() + bar.get_height()/2,\n             f\"{val:.1f} pp\", va='center', fontsize=10, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Biggest drop-off: {biggest_drop_stage} ({biggest_drop_val:.1f} percentage points)\")\nprint(f\"Overall completion rate: {fmt_pct(values[4])}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s4-funnel-by-channel",
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# 4b. Funnel by Channel -- Which channels retain viewers best?\n# ---------------------------------------------------------------------------\n# Same ratio formula: SUM(engagement_count) / SUM(views) * 100\nfunnel_by_ch = query(\"\"\"\n    SELECT\n        channel,\n        ROUND(SUM(video_engagement_1)   * 100.0 / NULLIF(SUM(video_view), 0), 1) AS started,\n        ROUND(SUM(video_engagement_25)  * 100.0 / NULLIF(SUM(video_view), 0), 1) AS reached_25,\n        ROUND(SUM(video_engagement_50)  * 100.0 / NULLIF(SUM(video_view), 0), 1) AS reached_50,\n        ROUND(SUM(video_engagement_75)  * 100.0 / NULLIF(SUM(video_view), 0), 1) AS reached_75,\n        ROUND(SUM(video_engagement_100) * 100.0 / NULLIF(SUM(video_view), 0), 1) AS completed,\n        SUM(video_view) AS total_views\n    FROM daily_analytics\n    WHERE video_view > 0\n    GROUP BY channel\n    ORDER BY completed DESC\n\"\"\")\n\n# Retention ratio: completed / started\nfunnel_by_ch['retention_ratio'] = (\n    funnel_by_ch['completed'] / funnel_by_ch['started'].replace(0, np.nan) * 100\n).round(1)\n\nprint(\"ENGAGEMENT FUNNEL BY CHANNEL\")\nprint(\"=\" * 80)\ndisplay(funnel_by_ch[['channel', 'started', 'reached_25', 'reached_50',\n                       'reached_75', 'completed', 'retention_ratio', 'total_views']])\n\n# Best and worst retention\nbest_ch = funnel_by_ch.iloc[0]['channel']\nworst_ch = funnel_by_ch.iloc[-1]['channel']\nFUNNEL['best_retention_channel'] = best_ch\nFUNNEL['worst_retention_channel'] = worst_ch\n\nprint(f\"\\nBest retention: {best_ch} ({funnel_by_ch.iloc[0]['completed']:.1f}% completion)\")\nprint(f\"Lowest retention: {worst_ch} ({funnel_by_ch.iloc[-1]['completed']:.1f}% completion)\")"
  },
  {
   "cell_type": "markdown",
   "id": "s4-takeaway",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "\n",
    "> **THE BIG WIN:** I identified that the largest viewer drop-off occurs at the **[biggest_drop_stage]** mark, with **[biggest_drop_val]** percentage points lost. Only **[completion_rate]** of viewers complete videos. This led me to recommend stronger opening hooks, front-loading key messages in the first 25% of content, and establishing optimal duration guidelines. Channel **[best_channel]** showed the highest retention, which I used as a benchmark for content production standards across all accounts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Content Strategy -- Duration Sweet Spot\n",
    "\n",
    "**Business Question:** What video length drives the best engagement? What should we tell content producers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s5-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 5. Duration Sweet Spot Analysis\n",
    "# ---------------------------------------------------------------------------\n",
    "duration = query(\"\"\"\n",
    "    SELECT\n",
    "        CASE\n",
    "            WHEN video_duration <= 60   THEN '1. 0-1 min'\n",
    "            WHEN video_duration <= 180  THEN '2. 1-3 min'\n",
    "            WHEN video_duration <= 300  THEN '3. 3-5 min'\n",
    "            WHEN video_duration <= 600  THEN '4. 5-10 min'\n",
    "            WHEN video_duration <= 1200 THEN '5. 10-20 min'\n",
    "            WHEN video_duration <= 1800 THEN '6. 20-30 min'\n",
    "            ELSE '7. 30+ min'\n",
    "        END AS duration_bucket,\n",
    "        COUNT(DISTINCT video_id)                AS num_videos,\n",
    "        SUM(video_view)                         AS total_views,\n",
    "        ROUND(AVG(engagement_score), 1)         AS avg_engagement,\n",
    "        ROUND(AVG(video_engagement_100), 1)     AS completion_rate,\n",
    "        ROUND(AVG(video_engagement_50), 1)      AS halfway_rate\n",
    "    FROM daily_analytics\n",
    "    WHERE video_view > 0 AND video_duration > 0\n",
    "    GROUP BY 1\n",
    "    ORDER BY 1\n",
    "\"\"\")\n",
    "\n",
    "# Identify sweet spot\n",
    "sweet_spot_idx  = duration['completion_rate'].idxmax()\n",
    "sweet_spot      = duration.loc[sweet_spot_idx]\n",
    "worst_idx       = duration['completion_rate'].idxmin()\n",
    "worst_bucket    = duration.loc[worst_idx]\n",
    "penalty         = sweet_spot['completion_rate'] - worst_bucket['completion_rate']\n",
    "\n",
    "# Production vs performance mismatch\n",
    "most_produced_idx = duration['num_videos'].idxmax()\n",
    "most_produced     = duration.loc[most_produced_idx]\n",
    "mismatch = most_produced['duration_bucket'] != sweet_spot['duration_bucket']\n",
    "\n",
    "CONTENT = {\n",
    "    'sweet_spot_bucket':     sweet_spot['duration_bucket'],\n",
    "    'sweet_spot_completion': sweet_spot['completion_rate'],\n",
    "    'sweet_spot_engagement': sweet_spot['avg_engagement'],\n",
    "    'worst_bucket':          worst_bucket['duration_bucket'],\n",
    "    'worst_completion':      worst_bucket['completion_rate'],\n",
    "    'penalty_pp':            penalty,\n",
    "    'most_produced_bucket':  most_produced['duration_bucket'],\n",
    "    'production_mismatch':   mismatch,\n",
    "}\n",
    "\n",
    "# -- Charts --\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Completion rate by bucket\n",
    "colors_comp = [C['success'] if i == sweet_spot_idx else C['blue_light'] for i in range(len(duration))]\n",
    "axes[0].bar(range(len(duration)), duration['completion_rate'], color=colors_comp, edgecolor='white')\n",
    "axes[0].set_xticks(range(len(duration)))\n",
    "axes[0].set_xticklabels([b.split('. ')[1] for b in duration['duration_bucket']], rotation=45, ha='right')\n",
    "axes[0].set_ylabel('Completion Rate (%)')\n",
    "axes[0].set_title('Completion Rate by Duration', fontsize=12, fontweight='bold')\n",
    "for i, v in enumerate(duration['completion_rate']):\n",
    "    axes[0].text(i, v + 0.5, fmt_pct(v), ha='center', fontsize=8)\n",
    "\n",
    "# Engagement score by bucket\n",
    "colors_eng = [C['success'] if i == duration['avg_engagement'].idxmax() else C['purple_light']\n",
    "              for i in range(len(duration))]\n",
    "axes[1].bar(range(len(duration)), duration['avg_engagement'], color=colors_eng, edgecolor='white')\n",
    "axes[1].set_xticks(range(len(duration)))\n",
    "axes[1].set_xticklabels([b.split('. ')[1] for b in duration['duration_bucket']], rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Engagement Score (%)')\n",
    "axes[1].set_title('Engagement Score by Duration', fontsize=12, fontweight='bold')\n",
    "for i, v in enumerate(duration['avg_engagement']):\n",
    "    axes[1].text(i, v + 0.5, fmt_pct(v), ha='center', fontsize=8)\n",
    "\n",
    "# Views distribution (what are we producing?)\n",
    "colors_prod = [C['warn'] if i == most_produced_idx else C['gray_light'] for i in range(len(duration))]\n",
    "axes[2].bar(range(len(duration)), duration['num_videos'], color=colors_prod, edgecolor='white')\n",
    "axes[2].set_xticks(range(len(duration)))\n",
    "axes[2].set_xticklabels([b.split('. ')[1] for b in duration['duration_bucket']], rotation=45, ha='right')\n",
    "axes[2].set_ylabel('Number of Videos')\n",
    "axes[2].set_title('Production Volume by Duration', fontsize=12, fontweight='bold')\n",
    "for i, v in enumerate(duration['num_videos']):\n",
    "    axes[2].text(i, v + 0.5, fmt_num(v), ha='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nDURATION PERFORMANCE BREAKDOWN\")\n",
    "print(\"=\" * 80)\n",
    "display(duration)\n",
    "\n",
    "print(f\"\\nSweet spot: {sweet_spot['duration_bucket']} ({fmt_pct(sweet_spot['completion_rate'])} completion)\")\n",
    "print(f\"Worst performer: {worst_bucket['duration_bucket']} ({fmt_pct(worst_bucket['completion_rate'])} completion)\")\n",
    "print(f\"Penalty for wrong duration: {penalty:.1f} percentage points\")\n",
    "if mismatch:\n",
    "    print(f\"\\nPRODUCTION MISMATCH: We produce most videos in '{most_produced['duration_bucket']}' \"\n",
    "          f\"but the sweet spot is '{sweet_spot['duration_bucket']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s5-takeaway",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "\n",
    "> Analysis of all videos revealed a clear sweet spot at **[sweet_spot_bucket]** with a **[sweet_spot_completion]** completion rate -- **[penalty_pp]** percentage points higher than the worst-performing bucket (**[worst_bucket]**). I discovered a production-vs-performance mismatch: we produce the most videos at **[most_produced_bucket]**, but the data shows **[sweet_spot_bucket]** delivers the best engagement. This insight led to updated content guidelines for producers, recommending front-loading value and keeping content within the optimal range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s6-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Top Performing Content & Content Gaps\n",
    "\n",
    "**Business Question:** Which videos are our greatest hits? Which have high impressions but low play rates (thumbnail/title problems)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s6-top-views",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 6a. Top 20 Videos by Total Views\n",
    "# ---------------------------------------------------------------------------\n",
    "top_by_views = query(\"\"\"\n",
    "    SELECT\n",
    "        channel,\n",
    "        video_id,\n",
    "        MAX(name) AS video_name,\n",
    "        SUM(video_view) AS total_views,\n",
    "        ROUND(AVG(engagement_score), 1) AS avg_engagement,\n",
    "        ROUND(AVG(video_engagement_100), 1) AS completion_rate\n",
    "    FROM daily_analytics\n",
    "    WHERE video_view > 0\n",
    "    GROUP BY channel, video_id\n",
    "    ORDER BY total_views DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "print(\"TOP 20 VIDEOS BY TOTAL VIEWS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "labels = [truncate_name(n) for n in top_by_views['video_name']]\n",
    "y_pos = range(len(labels) - 1, -1, -1)\n",
    "\n",
    "ax.barh(y_pos, top_by_views['total_views'], color=C['blue_light'], edgecolor=C['blue'], linewidth=0.5)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(labels[::-1], fontsize=8)\n",
    "ax.set_xlabel('Total Views')\n",
    "ax.set_title('Top 20 Videos by Total Views', fontsize=14, fontweight='bold')\n",
    "ax.xaxis.set_major_formatter(mticker.FuncFormatter(lambda v, _: fmt_num(v)))\n",
    "\n",
    "for i, (views, eng) in enumerate(zip(top_by_views['total_views'][::-1],\n",
    "                                      top_by_views['avg_engagement'][::-1])):\n",
    "    ax.text(views + max(top_by_views['total_views']) * 0.01, i,\n",
    "            f\"{fmt_num(views)} | {fmt_pct(eng)} eng.\", va='center', fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s6-top-engagement",
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# 6b. Top 20 Videos by Engagement (min 100 views for significance)\n# ---------------------------------------------------------------------------\ntop_by_eng = query(\"\"\"\n    SELECT\n        channel,\n        video_id,\n        MAX(name) AS video_name,\n        SUM(video_view) AS total_views,\n        ROUND(AVG(engagement_score), 1) AS avg_engagement,\n        ROUND(AVG(video_engagement_100), 1) AS completion_rate\n    FROM daily_analytics\n    WHERE video_view > 0\n    GROUP BY channel, video_id\n    HAVING SUM(video_view) >= 100\n    ORDER BY avg_engagement DESC\n    LIMIT 20\n\"\"\")\n\nprint(\"TOP 20 VIDEOS BY ENGAGEMENT (min 100 views)\")\nprint(\"=\" * 80)\n\nfig, ax = plt.subplots(figsize=(14, 8))\nlabels = [truncate_name(n, 40) for n in top_by_eng['video_name']]\ny_pos = range(len(labels) - 1, -1, -1)\n\nax.barh(y_pos, top_by_eng['avg_engagement'], color=C['success_light'],\n        edgecolor=C['success'], linewidth=0.5)\nax.set_yticks(y_pos)\nax.set_yticklabels(labels[::-1], fontsize=8)\nax.set_xlabel('Engagement Score (%)')\nax.set_xlim(0, 110)\nax.set_title('Top 20 Videos by Engagement Score (min 100 views)', fontsize=14, fontweight='bold')\n\nfor i, (eng, views) in enumerate(zip(top_by_eng['avg_engagement'][::-1],\n                                      top_by_eng['total_views'][::-1])):\n    ax.text(eng + 1, i, f\"{fmt_pct(eng)} | {fmt_num(views)} views\", va='center', fontsize=7)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s6-problems-gems",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 6c. Problem Videos & Hidden Gems\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Problem videos: high impressions but low play rate (thumbnail/title issue)\n",
    "problem_videos = query(\"\"\"\n",
    "    SELECT\n",
    "        channel,\n",
    "        video_id,\n",
    "        MAX(name) AS video_name,\n",
    "        SUM(video_impression) AS impressions,\n",
    "        SUM(video_view) AS views,\n",
    "        ROUND(SUM(video_view) * 100.0 / NULLIF(SUM(video_impression), 0), 1) AS play_rate,\n",
    "        ROUND(AVG(engagement_score), 1) AS avg_engagement\n",
    "    FROM daily_analytics\n",
    "    WHERE video_impression > 0\n",
    "    GROUP BY channel, video_id\n",
    "    HAVING SUM(video_impression) >= 200\n",
    "       AND SUM(video_view) * 100.0 / NULLIF(SUM(video_impression), 0) < 30\n",
    "    ORDER BY impressions DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "print(\"PROBLEM VIDEOS: High Impressions, Low Play Rate (<30%)\")\n",
    "print(\"These videos are being shown but failing to convert -- thumbnail/title issue\")\n",
    "print(\"=\" * 80)\n",
    "if len(problem_videos) > 0:\n",
    "    display(problem_videos)\n",
    "else:\n",
    "    print(\"No videos found matching these criteria.\")\n",
    "\n",
    "# Hidden gems: high engagement but low view count\n",
    "hidden_gems = query(\"\"\"\n",
    "    SELECT\n",
    "        channel,\n",
    "        video_id,\n",
    "        MAX(name) AS video_name,\n",
    "        SUM(video_view) AS total_views,\n",
    "        ROUND(AVG(engagement_score), 1) AS avg_engagement,\n",
    "        ROUND(AVG(video_engagement_100), 1) AS completion_rate\n",
    "    FROM daily_analytics\n",
    "    WHERE video_view > 0\n",
    "    GROUP BY channel, video_id\n",
    "    HAVING AVG(engagement_score) > 70\n",
    "       AND SUM(video_view) < 200\n",
    "       AND SUM(video_view) >= 20\n",
    "    ORDER BY avg_engagement DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nHIDDEN GEMS: High Engagement (>70%), Low Views (<200)\")\n",
    "print(\"Underpromotion opportunity -- these resonate but are not being discovered\")\n",
    "print(\"=\" * 80)\n",
    "if len(hidden_gems) > 0:\n",
    "    display(hidden_gems)\n",
    "else:\n",
    "    print(\"No hidden gems found matching these criteria (try adjusting thresholds).\")\n",
    "\n",
    "TOPVIDS = {\n",
    "    'top_video_name':    top_by_views.iloc[0]['video_name'] if len(top_by_views) > 0 else 'N/A',\n",
    "    'top_video_views':   top_by_views.iloc[0]['total_views'] if len(top_by_views) > 0 else 0,\n",
    "    'problem_video_count':  len(problem_videos),\n",
    "    'hidden_gem_count':     len(hidden_gems),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s6-takeaway",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "\n",
    "> I created a content performance intelligence framework analogous to Search Analytics success/relevance analysis. Our top video achieved **[top_video_views]** views. I identified **[problem_video_count]** \"problem videos\" with high impressions but play rates under 30% -- these have discoverability but fail to convert, indicating thumbnail or title issues. Conversely, I found **[hidden_gem_count]** \"hidden gems\" with engagement above 70% but fewer than 200 views -- underpromotion opportunities. This framework gave content producers data-driven feedback for the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s7-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Channel/Account Performance Comparison\n",
    "\n",
    "**Business Question:** How do our 11 accounts perform? Where should we invest or consolidate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 7a. Channel Performance Comparison\n",
    "# ---------------------------------------------------------------------------\n",
    "ch_perf = query(\"\"\"\n",
    "    SELECT\n",
    "        channel,\n",
    "        COUNT(DISTINCT video_id)                              AS num_videos,\n",
    "        SUM(video_view)                                       AS total_views,\n",
    "        ROUND(SUM(video_view) * 1.0 / NULLIF(COUNT(DISTINCT video_id), 0), 0) AS views_per_video,\n",
    "        ROUND(AVG(engagement_score), 1)                       AS avg_engagement,\n",
    "        ROUND(AVG(video_engagement_100), 1)                   AS completion_rate\n",
    "    FROM daily_analytics\n",
    "    WHERE video_view > 0\n",
    "    GROUP BY channel\n",
    "    ORDER BY total_views DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"CHANNEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "display(ch_perf)\n",
    "\n",
    "# -- Chart: Views vs Engagement scatter --\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "\n",
    "# Medians for quadrant lines\n",
    "med_views = ch_perf['total_views'].median()\n",
    "med_eng   = ch_perf['avg_engagement'].median()\n",
    "\n",
    "ax.axhline(y=med_eng, color=C['gray'], linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=med_views, color=C['gray'], linestyle='--', alpha=0.5)\n",
    "\n",
    "scatter = ax.scatter(ch_perf['total_views'], ch_perf['avg_engagement'],\n",
    "                     s=ch_perf['num_videos'] * 3, c=C['blue'], alpha=0.7, edgecolors=C['neutral'])\n",
    "\n",
    "for _, row in ch_perf.iterrows():\n",
    "    ax.annotate(row['channel'], (row['total_views'], row['avg_engagement']),\n",
    "                textcoords='offset points', xytext=(8, 4), fontsize=8)\n",
    "\n",
    "# Quadrant labels\n",
    "ax.text(0.02, 0.98, 'OPPORTUNITIES\\n(High Eng, Low Reach)', transform=ax.transAxes,\n",
    "        fontsize=8, va='top', ha='left', color=C['warn'], fontstyle='italic')\n",
    "ax.text(0.98, 0.98, 'STARS\\n(High Eng, High Reach)', transform=ax.transAxes,\n",
    "        fontsize=8, va='top', ha='right', color=C['success'], fontstyle='italic')\n",
    "ax.text(0.02, 0.02, 'RECONSIDER\\n(Low Eng, Low Reach)', transform=ax.transAxes,\n",
    "        fontsize=8, va='bottom', ha='left', color=C['fail'], fontstyle='italic')\n",
    "ax.text(0.98, 0.02, 'CASH COWS\\n(Low Eng, High Reach)', transform=ax.transAxes,\n",
    "        fontsize=8, va='bottom', ha='right', color=C['neutral'], fontstyle='italic')\n",
    "\n",
    "ax.set_xlabel('Total Views', fontsize=11)\n",
    "ax.set_ylabel('Avg Engagement Score (%)', fontsize=11)\n",
    "ax.set_title('BCG-Style Channel Matrix: Reach vs. Engagement\\n(bubble size = number of videos)',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.xaxis.set_major_formatter(mticker.FuncFormatter(lambda v, _: fmt_num(v)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 7b. BCG Classification & Consolidation Analysis\n",
    "# ---------------------------------------------------------------------------\n",
    "def classify_channel(row):\n",
    "    high_reach = row['total_views'] >= med_views\n",
    "    high_eng   = row['avg_engagement'] >= med_eng\n",
    "    if high_reach and high_eng:\n",
    "        return 'Star'\n",
    "    elif not high_reach and high_eng:\n",
    "        return 'Opportunity'\n",
    "    elif high_reach and not high_eng:\n",
    "        return 'Cash Cow'\n",
    "    else:\n",
    "        return 'Reconsider'\n",
    "\n",
    "ch_perf['bcg_category'] = ch_perf.apply(classify_channel, axis=1)\n",
    "\n",
    "print(\"BCG CHANNEL CLASSIFICATION\")\n",
    "print(\"=\" * 66)\n",
    "for cat in ['Star', 'Opportunity', 'Cash Cow', 'Reconsider']:\n",
    "    channels_in_cat = ch_perf[ch_perf['bcg_category'] == cat]['channel'].tolist()\n",
    "    print(f\"  {cat:15} : {', '.join(channels_in_cat) if channels_in_cat else '(none)'}\")\n",
    "\n",
    "# Consolidation candidates\n",
    "reconsider = ch_perf[ch_perf['bcg_category'] == 'Reconsider']\n",
    "if len(reconsider) > 0:\n",
    "    reconsider_views = reconsider['total_views'].sum()\n",
    "    total_views_all  = ch_perf['total_views'].sum()\n",
    "    reconsider_pct   = reconsider_views / total_views_all * 100 if total_views_all > 0 else 0\n",
    "    print(f\"\\nConsolidation candidates represent {fmt_pct(reconsider_pct)} of total views\")\n",
    "    print(f\"({fmt_num(reconsider_views)} out of {fmt_num(total_views_all)} views)\")\n",
    "else:\n",
    "    reconsider_pct = 0\n",
    "\n",
    "CHANNELS = {\n",
    "    'total_channels':      len(ch_perf),\n",
    "    'stars':               ch_perf[ch_perf['bcg_category'] == 'Star']['channel'].tolist(),\n",
    "    'opportunities':       ch_perf[ch_perf['bcg_category'] == 'Opportunity']['channel'].tolist(),\n",
    "    'cash_cows':           ch_perf[ch_perf['bcg_category'] == 'Cash Cow']['channel'].tolist(),\n",
    "    'reconsider':          ch_perf[ch_perf['bcg_category'] == 'Reconsider']['channel'].tolist(),\n",
    "    'best_engagement_ch':  ch_perf.loc[ch_perf['avg_engagement'].idxmax(), 'channel'],\n",
    "    'best_reach_ch':       ch_perf.iloc[0]['channel'],\n",
    "    'consolidation_pct':   reconsider_pct,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s7-takeaway",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "\n",
    "> Using a BCG-style matrix, I categorized **[total_channels]** Brightcove accounts by reach and engagement. **[stars]** emerged as Stars (invest more), **[opportunities]** as Opportunities (high engagement, promote more), **[cash_cows]** as Cash Cows (maintain), and **[reconsider]** as Reconsider candidates. The Reconsider channels accounted for only **[consolidation_pct]** of total views, supporting a consolidation recommendation that would reduce operational overhead while concentrating resources on high-performing channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s8-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Device & Platform Strategy\n",
    "\n",
    "**Business Question:** How are employees watching videos? Is mobile growing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s8-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 8a. Device Breakdown\n",
    "# ---------------------------------------------------------------------------\n",
    "devices = query(\"\"\"\n",
    "    SELECT\n",
    "        SUM(views_desktop)  AS desktop,\n",
    "        SUM(views_mobile)   AS mobile,\n",
    "        SUM(views_tablet)   AS tablet,\n",
    "        SUM(views_other)    AS other_device\n",
    "    FROM daily_analytics\n",
    "\"\"\").iloc[0]\n",
    "\n",
    "device_data = {k: int(v) for k, v in devices.items() if v and v > 0}\n",
    "total_device = sum(device_data.values())\n",
    "device_pcts  = {k: v / total_device * 100 for k, v in device_data.items()}\n",
    "\n",
    "DEVICE['breakdown'] = device_pcts\n",
    "DEVICE['total'] = total_device\n",
    "\n",
    "# -- Charts: Pie + Bar --\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=FIGSIZE)\n",
    "\n",
    "device_colors = [C['blue'], C['fail'], C['success'], C['gray']]\n",
    "device_labels_map = {'desktop': 'Desktop', 'mobile': 'Mobile', 'tablet': 'Tablet', 'other_device': 'Other'}\n",
    "labels_nice = [device_labels_map.get(k, k) for k in device_data.keys()]\n",
    "\n",
    "ax1.pie(device_data.values(), labels=labels_nice, autopct='%1.1f%%',\n",
    "        colors=device_colors[:len(device_data)], startangle=90, textprops={'fontsize': 10})\n",
    "ax1.set_title('Views by Device Type', fontsize=13, fontweight='bold')\n",
    "\n",
    "bars = ax2.bar(labels_nice, device_data.values(), color=device_colors[:len(device_data)],\n",
    "               edgecolor='white')\n",
    "ax2.set_ylabel('Total Views')\n",
    "ax2.set_title('Total Views by Device', fontsize=13, fontweight='bold')\n",
    "ax2.yaxis.set_major_formatter(mticker.FuncFormatter(lambda v, _: fmt_num(v)))\n",
    "for bar, (k, v) in zip(bars, device_data.items()):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + total_device * 0.01,\n",
    "             f\"{v/total_device*100:.1f}%\", ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s8-mobile-trend",
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# 8b. Mobile Percentage Trend (exclude current incomplete month)\n# ---------------------------------------------------------------------------\ndevice_trend = query(\"\"\"\n    SELECT\n        DATE_TRUNC('month', date) AS month,\n        SUM(views_desktop) AS desktop,\n        SUM(views_mobile)  AS mobile,\n        SUM(views_tablet)  AS tablet,\n        SUM(video_view)    AS total_views\n    FROM daily_analytics\n    WHERE video_view > 0\n      AND DATE_TRUNC('month', date) < DATE_TRUNC('month', CURRENT_DATE)\n    GROUP BY 1\n    ORDER BY 1\n\"\"\")\n\ndevice_trend['mobile_pct'] = (device_trend['mobile'] / device_trend['total_views'] * 100).round(1)\ndevice_trend['desktop_pct'] = (device_trend['desktop'] / device_trend['total_views'] * 100).round(1)\n\nfig, ax = plt.subplots(figsize=FIGSIZE)\n\nmonths_str = device_trend['month'].dt.strftime('%Y-%m').tolist()\nax.plot(months_str, device_trend['mobile_pct'], marker='o', linewidth=2, markersize=6,\n        color=C['fail'], label='Mobile %')\nax.plot(months_str, device_trend['desktop_pct'], marker='s', linewidth=2, markersize=6,\n        color=C['blue'], label='Desktop %')\nax.axhline(y=30, color=C['gray'], linestyle='--', alpha=0.6, label='30% Mobile Threshold')\n\nax.set_xlabel('Month', fontsize=11)\nax.set_ylabel('Percentage of Views (%)', fontsize=11)\nax.set_title('Device Usage Trend Over Time (complete months only)',\n             fontsize=14, fontweight='bold')\nax.legend(loc='best', framealpha=0.9)\nax.tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n# Mobile trend summary\nif len(device_trend) >= 2:\n    first_mobile = device_trend['mobile_pct'].iloc[0]\n    last_mobile  = device_trend['mobile_pct'].iloc[-1]\n    mobile_growth = last_mobile - first_mobile\n    DEVICE['mobile_first'] = first_mobile\n    DEVICE['mobile_last']  = last_mobile\n    DEVICE['mobile_growth_pp'] = mobile_growth\n    DEVICE['above_30_threshold'] = last_mobile > 30\n    print(f\"Mobile trend: {first_mobile:.1f}% -> {last_mobile:.1f}% ({mobile_growth:+.1f} pp)\")\n    if last_mobile > 30:\n        print(\"Mobile exceeds 30% threshold -- mobile-first strategy recommended\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s8-device-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 8c. Device Type vs Engagement\n",
    "# ---------------------------------------------------------------------------\n",
    "# We derive per-device engagement by comparing channels with high/low mobile share\n",
    "device_eng = query(\"\"\"\n",
    "    SELECT\n",
    "        channel,\n",
    "        ROUND(SUM(views_mobile)*100.0 / NULLIF(SUM(video_view),0), 1) AS mobile_pct,\n",
    "        ROUND(AVG(engagement_score), 1)         AS avg_engagement,\n",
    "        ROUND(AVG(video_engagement_100), 1)     AS completion_rate,\n",
    "        SUM(video_view)                         AS total_views\n",
    "    FROM daily_analytics\n",
    "    WHERE video_view > 0\n",
    "    GROUP BY channel\n",
    "    HAVING SUM(video_view) >= 100\n",
    "    ORDER BY mobile_pct DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"DEVICE MIX & ENGAGEMENT BY CHANNEL\")\n",
    "print(\"=\" * 66)\n",
    "display(device_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s8-takeaway",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "\n",
    "> Desktop accounts for **[desktop_pct]** of views, mobile **[mobile_pct]**, and tablet **[tablet_pct]**. Mobile viewing **[grew/declined]** from **[mobile_first_pct]** to **[mobile_last_pct]** over the measurement period (**[mobile_growth_pp]** percentage points). **[If above 30%: This exceeds the 30% threshold, justifying a mobile-first content strategy including larger on-screen text, subtitles, and mobile-optimized thumbnails.]** **[If below 30%: Desktop-first remains appropriate, though the growth trend should be monitored quarterly.]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s9-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Content Lifecycle & Freshness\n",
    "\n",
    "**Business Question:** Which content is stale? What is the archival opportunity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s9-stale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 9a. Stale Content (not viewed in 180+ days)\n",
    "# ---------------------------------------------------------------------------\n",
    "stale = query(\"\"\"\n",
    "    SELECT\n",
    "        channel,\n",
    "        video_id,\n",
    "        MAX(name)                                               AS video_name,\n",
    "        MAX(dt_last_viewed)                                     AS last_viewed,\n",
    "        SUM(video_view)                                         AS lifetime_views,\n",
    "        MAX(created_at)::DATE                                   AS created_date,\n",
    "        ROUND(MAX(video_duration) / 60.0, 1)                    AS duration_min,\n",
    "        DATEDIFF('day', MAX(dt_last_viewed)::DATE, CURRENT_DATE) AS days_since_viewed\n",
    "    FROM daily_analytics\n",
    "    WHERE dt_last_viewed IS NOT NULL\n",
    "    GROUP BY channel, video_id\n",
    "    HAVING DATEDIFF('day', MAX(dt_last_viewed)::DATE, CURRENT_DATE) > 180\n",
    "    ORDER BY lifetime_views DESC\n",
    "\"\"\")\n",
    "\n",
    "print(f\"STALE CONTENT: {len(stale)} videos not viewed in 180+ days\")\n",
    "print(\"=\" * 80)\n",
    "if len(stale) > 0:\n",
    "    display(stale.head(20))\n",
    "    total_stale_hours = stale['duration_min'].sum() / 60\n",
    "    print(f\"\\nTotal stale video duration: {total_stale_hours:,.1f} hours of content\")\n",
    "    print(f\"Total lifetime views of stale content: {fmt_num(stale['lifetime_views'].sum())}\")\n",
    "else:\n",
    "    print(\"No stale content found (all videos viewed within 180 days).\")\n",
    "    total_stale_hours = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s9-age-distribution",
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# 9b. Content Age Distribution\n# ---------------------------------------------------------------------------\n# First aggregate per video, then bucket by age (avoids aggregate in GROUP BY)\nage_dist = query(\"\"\"\n    WITH video_summary AS (\n        SELECT\n            video_id,\n            MAX(created_at)::DATE                                   AS created_date,\n            SUM(video_view)                                         AS total_views,\n            AVG(engagement_score)                                   AS avg_engagement\n        FROM daily_analytics\n        WHERE created_at IS NOT NULL AND video_view > 0\n        GROUP BY video_id\n    )\n    SELECT\n        CASE\n            WHEN DATEDIFF('day', created_date, CURRENT_DATE) <= 90  THEN '1. <3 months'\n            WHEN DATEDIFF('day', created_date, CURRENT_DATE) <= 180 THEN '2. 3-6 months'\n            WHEN DATEDIFF('day', created_date, CURRENT_DATE) <= 365 THEN '3. 6-12 months'\n            WHEN DATEDIFF('day', created_date, CURRENT_DATE) <= 730 THEN '4. 1-2 years'\n            ELSE '5. 2+ years'\n        END AS content_age,\n        COUNT(DISTINCT video_id) AS num_videos,\n        SUM(total_views)         AS total_views,\n        ROUND(AVG(avg_engagement), 1) AS avg_engagement\n    FROM video_summary\n    GROUP BY 1\n    ORDER BY 1\n\"\"\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=FIGSIZE)\n\nage_labels = [a.split('. ')[1] for a in age_dist['content_age']]\n\nax1.bar(age_labels, age_dist['num_videos'], color=C['blue_light'], edgecolor=C['blue'])\nax1.set_ylabel('Number of Videos')\nax1.set_title('Content Age Distribution', fontsize=12, fontweight='bold')\nax1.tick_params(axis='x', rotation=45)\nfor i, v in enumerate(age_dist['num_videos']):\n    ax1.text(i, v + max(age_dist['num_videos']) * 0.02, fmt_num(v), ha='center', fontsize=9)\n\nax2.bar(age_labels, age_dist['avg_engagement'], color=C['purple_light'], edgecolor=C['purple'])\nax2.set_ylabel('Avg Engagement Score (%)')\nax2.set_title('Engagement by Content Age', fontsize=12, fontweight='bold')\nax2.tick_params(axis='x', rotation=45)\nfor i, v in enumerate(age_dist['avg_engagement']):\n    ax2.text(i, v + 0.5, fmt_pct(v), ha='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"CONTENT AGE DISTRIBUTION\")\nprint(\"=\" * 66)\ndisplay(age_dist)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s9-recent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 9c. Recently Created: High Performers vs Underperformers\n",
    "# ---------------------------------------------------------------------------\n",
    "recent_created = query(\"\"\"\n",
    "    SELECT\n",
    "        channel,\n",
    "        video_id,\n",
    "        MAX(name) AS video_name,\n",
    "        MAX(created_at)::DATE AS created_date,\n",
    "        SUM(video_view) AS total_views,\n",
    "        ROUND(AVG(engagement_score), 1) AS avg_engagement,\n",
    "        ROUND(AVG(video_engagement_100), 1) AS completion_rate\n",
    "    FROM daily_analytics\n",
    "    WHERE created_at IS NOT NULL\n",
    "      AND DATEDIFF('day', created_at::DATE, CURRENT_DATE) <= 90\n",
    "      AND video_view > 0\n",
    "    GROUP BY channel, video_id\n",
    "    ORDER BY total_views DESC\n",
    "\"\"\")\n",
    "\n",
    "if len(recent_created) > 0:\n",
    "    median_views = recent_created['total_views'].median()\n",
    "    high_perf = recent_created[recent_created['total_views'] > median_views * 2].head(10)\n",
    "    low_perf  = recent_created[recent_created['total_views'] < median_views * 0.5].tail(10)\n",
    "\n",
    "    print(\"RECENTLY CREATED HIGH PERFORMERS (last 90 days, above 2x median views)\")\n",
    "    print(\"=\" * 80)\n",
    "    display(high_perf[['channel', 'video_name', 'total_views', 'avg_engagement', 'completion_rate']])\n",
    "\n",
    "    print(f\"\\nRECENTLY CREATED UNDERPERFORMERS (last 90 days, below 0.5x median views)\")\n",
    "    print(\"=\" * 80)\n",
    "    display(low_perf[['channel', 'video_name', 'total_views', 'avg_engagement', 'completion_rate']])\n",
    "else:\n",
    "    print(\"No recently created content found in the last 90 days.\")\n",
    "\n",
    "LIFECYCLE = {\n",
    "    'stale_count':       len(stale),\n",
    "    'stale_hours':       total_stale_hours,\n",
    "    'stale_lifetime_views': stale['lifetime_views'].sum() if len(stale) > 0 else 0,\n",
    "    'recent_count':      len(recent_created),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s9-takeaway",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "\n",
    "> I identified **[stale_count]** videos not viewed in over 180 days, representing **[stale_hours]** hours of stored content. These had **[stale_lifetime_views]** lifetime views, indicating they were once valuable but are now candidates for archival. Archiving stale content reduces storage costs and improves content discoverability for active videos. I also analyzed recently created content to identify early high performers vs underperformers, enabling faster feedback loops for content producers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s10-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Regional & Temporal Patterns\n",
    "\n",
    "**Business Question:** When and where are employees watching? What does that tell us about our global audience?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s10-regional",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 10a. Views by Country / Region\n",
    "# ---------------------------------------------------------------------------\n",
    "regional = query(\"\"\"\n",
    "    SELECT\n",
    "        COALESCE(country, 'Not Specified') AS country,\n",
    "        SUM(video_view) AS total_views,\n",
    "        COUNT(DISTINCT video_id) AS unique_videos,\n",
    "        ROUND(AVG(engagement_score), 1) AS avg_engagement,\n",
    "        ROUND(AVG(video_engagement_100), 1) AS completion_rate\n",
    "    FROM daily_analytics\n",
    "    WHERE video_view > 0\n",
    "    GROUP BY 1\n",
    "    HAVING SUM(video_view) >= 50\n",
    "    ORDER BY total_views DESC\n",
    "    LIMIT 15\n",
    "\"\"\")\n",
    "\n",
    "if len(regional) > 1 and regional.iloc[0]['country'] != 'Not Specified':\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    top_n = min(12, len(regional))\n",
    "    reg_top = regional.head(top_n)\n",
    "\n",
    "    ax1.barh(range(top_n - 1, -1, -1), reg_top['total_views'],\n",
    "             color=C['blue_light'], edgecolor=C['blue'])\n",
    "    ax1.set_yticks(range(top_n - 1, -1, -1))\n",
    "    ax1.set_yticklabels(reg_top['country'][::-1], fontsize=9)\n",
    "    ax1.set_xlabel('Total Views')\n",
    "    ax1.set_title('Views by Country/Region', fontsize=13, fontweight='bold')\n",
    "    ax1.xaxis.set_major_formatter(mticker.FuncFormatter(lambda v, _: fmt_num(v)))\n",
    "\n",
    "    ax2.barh(range(top_n - 1, -1, -1), reg_top['avg_engagement'],\n",
    "             color=C['success_light'], edgecolor=C['success'])\n",
    "    ax2.set_yticks(range(top_n - 1, -1, -1))\n",
    "    ax2.set_yticklabels(reg_top['country'][::-1], fontsize=9)\n",
    "    ax2.set_xlabel('Avg Engagement Score (%)')\n",
    "    ax2.set_title('Engagement by Country/Region', fontsize=13, fontweight='bold')\n",
    "    ax2.set_xlim(0, 100)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    REGIONAL['top_country'] = regional.iloc[0]['country']\n",
    "    REGIONAL['top_country_views'] = regional.iloc[0]['total_views']\n",
    "    REGIONAL['num_countries'] = len(regional)\n",
    "else:\n",
    "    print(\"Regional data not populated or single region -- skipping geographic chart.\")\n",
    "    REGIONAL['top_country'] = 'N/A'\n",
    "    REGIONAL['num_countries'] = 0\n",
    "\n",
    "print(\"\\nREGIONAL PERFORMANCE\")\n",
    "print(\"=\" * 66)\n",
    "display(regional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s10-temporal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 10b. Day-of-Week Patterns\n",
    "# ---------------------------------------------------------------------------\n",
    "dow = query(\"\"\"\n",
    "    SELECT\n",
    "        DAYOFWEEK(date) AS dow_num,\n",
    "        CASE DAYOFWEEK(date)\n",
    "            WHEN 0 THEN 'Sunday'\n",
    "            WHEN 1 THEN 'Monday'\n",
    "            WHEN 2 THEN 'Tuesday'\n",
    "            WHEN 3 THEN 'Wednesday'\n",
    "            WHEN 4 THEN 'Thursday'\n",
    "            WHEN 5 THEN 'Friday'\n",
    "            WHEN 6 THEN 'Saturday'\n",
    "        END AS day_name,\n",
    "        SUM(video_view) AS total_views,\n",
    "        ROUND(AVG(engagement_score), 1) AS avg_engagement\n",
    "    FROM daily_analytics\n",
    "    WHERE video_view > 0\n",
    "    GROUP BY 1, 2\n",
    "    ORDER BY 1\n",
    "\"\"\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=FIGSIZE)\n",
    "\n",
    "# Color weekdays vs weekends differently\n",
    "colors_dow = [C['gray_light'] if d in (0, 6) else C['blue_light'] for d in dow['dow_num']]\n",
    "\n",
    "ax1.bar(dow['day_name'], dow['total_views'], color=colors_dow, edgecolor=C['neutral'], linewidth=0.5)\n",
    "ax1.set_ylabel('Total Views')\n",
    "ax1.set_title('Views by Day of Week', fontsize=12, fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.yaxis.set_major_formatter(mticker.FuncFormatter(lambda v, _: fmt_num(v)))\n",
    "\n",
    "ax2.bar(dow['day_name'], dow['avg_engagement'], color=colors_dow, edgecolor=C['neutral'], linewidth=0.5)\n",
    "ax2.set_ylabel('Avg Engagement Score (%)')\n",
    "ax2.set_title('Engagement by Day of Week', fontsize=12, fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Weekday vs weekend\n",
    "weekday_views = dow[~dow['dow_num'].isin([0, 6])]['total_views'].sum()\n",
    "weekend_views = dow[dow['dow_num'].isin([0, 6])]['total_views'].sum()\n",
    "total_all = weekday_views + weekend_views\n",
    "weekday_pct = weekday_views / total_all * 100 if total_all > 0 else 0\n",
    "\n",
    "peak_day = dow.loc[dow['total_views'].idxmax(), 'day_name']\n",
    "\n",
    "REGIONAL['peak_day'] = peak_day\n",
    "REGIONAL['weekday_pct'] = weekday_pct\n",
    "\n",
    "print(f\"Weekday vs Weekend: {weekday_pct:.1f}% weekday / {100 - weekday_pct:.1f}% weekend\")\n",
    "print(f\"Peak viewing day: {peak_day}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s10-seasonality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 10c. Monthly Seasonality Patterns\n",
    "# ---------------------------------------------------------------------------\n",
    "seasonality = query(\"\"\"\n",
    "    SELECT\n",
    "        EXTRACT(MONTH FROM date) AS month_num,\n",
    "        CASE EXTRACT(MONTH FROM date)\n",
    "            WHEN 1 THEN 'Jan' WHEN 2 THEN 'Feb' WHEN 3 THEN 'Mar'\n",
    "            WHEN 4 THEN 'Apr' WHEN 5 THEN 'May' WHEN 6 THEN 'Jun'\n",
    "            WHEN 7 THEN 'Jul' WHEN 8 THEN 'Aug' WHEN 9 THEN 'Sep'\n",
    "            WHEN 10 THEN 'Oct' WHEN 11 THEN 'Nov' WHEN 12 THEN 'Dec'\n",
    "        END AS month_name,\n",
    "        SUM(video_view) AS total_views,\n",
    "        ROUND(AVG(engagement_score), 1) AS avg_engagement\n",
    "    FROM daily_analytics\n",
    "    WHERE video_view > 0\n",
    "    GROUP BY 1, 2\n",
    "    ORDER BY 1\n",
    "\"\"\")\n",
    "\n",
    "if len(seasonality) > 3:\n",
    "    fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "    ax.bar(seasonality['month_name'], seasonality['total_views'],\n",
    "           color=C['blue_light'], edgecolor=C['blue'])\n",
    "    ax.set_ylabel('Total Views')\n",
    "    ax.set_title('Seasonal Viewing Patterns (Aggregated Across Years)', fontsize=13, fontweight='bold')\n",
    "    ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda v, _: fmt_num(v)))\n",
    "    for i, v in enumerate(seasonality['total_views']):\n",
    "        ax.text(i, v + max(seasonality['total_views']) * 0.02, fmt_num(v),\n",
    "                ha='center', fontsize=8, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    peak_month = seasonality.loc[seasonality['total_views'].idxmax(), 'month_name']\n",
    "    low_month  = seasonality.loc[seasonality['total_views'].idxmin(), 'month_name']\n",
    "    REGIONAL['peak_month'] = peak_month\n",
    "    REGIONAL['low_month'] = low_month\n",
    "    print(f\"Peak month: {peak_month}\")\n",
    "    print(f\"Lowest month: {low_month}\")\n",
    "else:\n",
    "    REGIONAL['peak_month'] = 'N/A'\n",
    "    REGIONAL['low_month'] = 'N/A'\n",
    "    print(\"Insufficient data for seasonality analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s10-channel-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 10d. Channel Performance Variation by Region (top 5 countries x channels)\n",
    "# ---------------------------------------------------------------------------\n",
    "ch_region = query(\"\"\"\n",
    "    SELECT\n",
    "        COALESCE(country, 'Not Specified') AS country,\n",
    "        channel,\n",
    "        SUM(video_view) AS total_views,\n",
    "        ROUND(AVG(engagement_score), 1) AS avg_engagement\n",
    "    FROM daily_analytics\n",
    "    WHERE video_view > 0\n",
    "    GROUP BY 1, 2\n",
    "    HAVING SUM(video_view) >= 50\n",
    "    ORDER BY total_views DESC\n",
    "    LIMIT 30\n",
    "\"\"\")\n",
    "\n",
    "if len(ch_region) > 0 and ch_region.iloc[0]['country'] != 'Not Specified':\n",
    "    print(\"CHANNEL PERFORMANCE BY REGION (top combinations)\")\n",
    "    print(\"=\" * 66)\n",
    "    display(ch_region.head(20))\n",
    "else:\n",
    "    print(\"Regional x channel cross-analysis not available (country not populated).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s10-takeaway",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "\n",
    "> Viewing patterns show **[weekday_pct]** of consumption on business days, with **[peak_day]** as the peak viewing day. Seasonality analysis reveals **[peak_month]** as the highest-volume month and **[low_month]** as the lowest -- useful for content release planning and campaign timing. **[If regional data: The top viewing region is [top_country], and engagement varies by [X] percentage points across regions, suggesting potential for localized content strategies.]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s11-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11: Interview Cheat Sheet\n",
    "\n",
    "All key numbers compiled from the sections above, plus 7 STAR talking points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s11-cheatsheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "#  INTERVIEW CHEAT SHEET -- Compiled Reference Card\n",
    "# ===================================================================\n",
    "\n",
    "sep  = \"=\" * 72\n",
    "dash = \"-\" * 72\n",
    "\n",
    "print(sep)\n",
    "print(\"        VIDEO ANALYTICS -- INTERVIEW CHEAT SHEET\")\n",
    "print(sep)\n",
    "\n",
    "# --- SCOPE ---\n",
    "print(f\"\\n  SCOPE\")\n",
    "print(dash)\n",
    "print(f\"  Period:             {EXEC.get('period_start', 'N/A')} to {EXEC.get('period_end', 'N/A')} ({int(EXEC.get('period_days', 0))} days)\")\n",
    "print(f\"  Total Videos:       {fmt_num(EXEC.get('total_videos', 0))}\")\n",
    "print(f\"  Total Channels:     {fmt_num(EXEC.get('total_channels', 0))} (across 4 categories)\")\n",
    "print(f\"  Total Views:        {fmt_num(EXEC.get('total_views', 0))}\")\n",
    "print(f\"  Total Watch Hours:  {fmt_num(EXEC.get('total_watch_hours', 0))}\")\n",
    "print(f\"  Total Impressions:  {fmt_num(EXEC.get('total_impressions', 0))}\")\n",
    "\n",
    "# --- QUALITY METRICS ---\n",
    "print(f\"\\n  QUALITY METRICS\")\n",
    "print(dash)\n",
    "print(f\"  Engagement Score:   {fmt_pct(ENGAGEMENT.get('avg_engagement', 0))}\")\n",
    "print(f\"  Completion Rate:    {fmt_pct(ENGAGEMENT.get('completion_rate', 0))}\")\n",
    "print(f\"  Halfway Rate:       {fmt_pct(ENGAGEMENT.get('halfway_rate', 0))}\")\n",
    "print(f\"  Play Rate:          {fmt_pct(EXEC.get('play_rate', 0))}  (views / impressions)\")\n",
    "print(f\"  Trend:              {ENGAGEMENT.get('trend', 'N/A')}\")\n",
    "\n",
    "# --- ENGAGEMENT FUNNEL ---\n",
    "print(f\"\\n  ENGAGEMENT FUNNEL\")\n",
    "print(dash)\n",
    "print(f\"  Started (1%):       {fmt_pct(FUNNEL.get('started', 0))}\")\n",
    "print(f\"  Reached 25%:        {fmt_pct(FUNNEL.get('reached_25', 0))}\")\n",
    "print(f\"  Reached 50%:        {fmt_pct(FUNNEL.get('reached_50', 0))}\")\n",
    "print(f\"  Reached 75%:        {fmt_pct(FUNNEL.get('reached_75', 0))}\")\n",
    "print(f\"  Completed (100%):   {fmt_pct(FUNNEL.get('completed', 0))}\")\n",
    "print(f\"  Biggest Drop-off:   {FUNNEL.get('biggest_drop_stage', 'N/A')} ({FUNNEL.get('biggest_drop_val', 0):.1f} pp)\")\n",
    "print(f\"  Best Retention:     {FUNNEL.get('best_retention_channel', 'N/A')}\")\n",
    "\n",
    "# --- CONTENT STRATEGY ---\n",
    "print(f\"\\n  CONTENT STRATEGY\")\n",
    "print(dash)\n",
    "print(f\"  Optimal Duration:   {CONTENT.get('sweet_spot_bucket', 'N/A')}\")\n",
    "print(f\"  Sweet Spot Compl.:  {fmt_pct(CONTENT.get('sweet_spot_completion', 0))}\")\n",
    "print(f\"  Worst Duration:     {CONTENT.get('worst_bucket', 'N/A')} ({fmt_pct(CONTENT.get('worst_completion', 0))})\")\n",
    "print(f\"  Duration Penalty:   {CONTENT.get('penalty_pp', 0):.1f} pp\")\n",
    "print(f\"  Production Mismatch: {'Yes' if CONTENT.get('production_mismatch') else 'No'}\")\n",
    "\n",
    "# --- CHANNEL PERFORMANCE ---\n",
    "print(f\"\\n  CHANNEL PERFORMANCE\")\n",
    "print(dash)\n",
    "print(f\"  Stars:              {', '.join(CHANNELS.get('stars', ['N/A']))}\")\n",
    "print(f\"  Opportunities:      {', '.join(CHANNELS.get('opportunities', ['N/A']))}\")\n",
    "print(f\"  Cash Cows:          {', '.join(CHANNELS.get('cash_cows', ['N/A']))}\")\n",
    "print(f\"  Reconsider:         {', '.join(CHANNELS.get('reconsider', ['N/A']))}\")\n",
    "print(f\"  Consolidation Opp.: {fmt_pct(CHANNELS.get('consolidation_pct', 0))} of views\")\n",
    "\n",
    "# --- DEVICE SPLIT ---\n",
    "print(f\"\\n  DEVICE SPLIT\")\n",
    "print(dash)\n",
    "for device, pct in DEVICE.get('breakdown', {}).items():\n",
    "    device_labels_map = {'desktop': 'Desktop', 'mobile': 'Mobile', 'tablet': 'Tablet', 'other_device': 'Other'}\n",
    "    print(f\"  {device_labels_map.get(device, device):18} {pct:.1f}%\")\n",
    "print(f\"  Mobile Trend:       {DEVICE.get('mobile_first', 0):.1f}% -> {DEVICE.get('mobile_last', 0):.1f}% ({DEVICE.get('mobile_growth_pp', 0):+.1f} pp)\")\n",
    "print(f\"  Above 30% Thresh:   {'Yes' if DEVICE.get('above_30_threshold') else 'No'}\")\n",
    "\n",
    "# --- CONTENT LIFECYCLE ---\n",
    "print(f\"\\n  CONTENT LIFECYCLE\")\n",
    "print(dash)\n",
    "print(f\"  Stale Videos (180d): {fmt_num(LIFECYCLE.get('stale_count', 0))}\")\n",
    "print(f\"  Stale Hours:         {LIFECYCLE.get('stale_hours', 0):,.1f} hours\")\n",
    "print(f\"  Recently Created:    {fmt_num(LIFECYCLE.get('recent_count', 0))} (last 90 days)\")\n",
    "\n",
    "# --- AUDIENCE ---\n",
    "print(f\"\\n  AUDIENCE\")\n",
    "print(dash)\n",
    "print(f\"  Top Region:         {REGIONAL.get('top_country', 'N/A')}\")\n",
    "print(f\"  Regions Tracked:    {REGIONAL.get('num_countries', 0)}\")\n",
    "print(f\"  Peak Day:           {REGIONAL.get('peak_day', 'N/A')}\")\n",
    "print(f\"  Weekday Viewing:    {REGIONAL.get('weekday_pct', 0):.1f}%\")\n",
    "print(f\"  Peak Month:         {REGIONAL.get('peak_month', 'N/A')}\")\n",
    "print(f\"  Low Month:          {REGIONAL.get('low_month', 'N/A')}\")\n",
    "\n",
    "print(f\"\\n{sep}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s11-star-stories",
   "metadata": {},
   "source": [
    "### 7 STAR Talking Points\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Building the Analytics Pipeline (Technical Leadership)\n",
    "\n",
    "**Situation:** The organization had 11 separate Brightcove video accounts across 4 business categories (internet/intranet, research, global wealth management, events) with no unified view of video performance. Each account was managed independently, making it impossible to benchmark or identify cross-cutting insights.\n",
    "\n",
    "**Task:** Build an automated ETL pipeline to consolidate all 11 accounts into a single analytics platform with daily granularity.\n",
    "\n",
    "**Action:** I designed and implemented a Python-based pipeline using the Brightcove Analytics API, storing data in DuckDB for fast analytical queries. The pipeline handles CMS metadata enrichment, incremental daily updates with overlap for data corrections, and video lifecycle tracking (dt_last_viewed). I also created executive dashboards for non-technical stakeholders.\n",
    "\n",
    "**Result:** Unified **[total_videos]** videos across **[total_channels]** channels into a single analytics platform tracking **[total_views]** views and **[total_watch_hours]** watch hours. For the first time, the organization had a cross-account view of video performance, enabling data-driven content strategy.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Engagement Funnel Optimization -- THE BIG WIN (Business Impact)\n",
    "\n",
    "**Situation:** Content producers were creating videos without understanding how viewers consumed them. There was no data on where viewers dropped off or why engagement varied across channels.\n",
    "\n",
    "**Task:** Identify specific viewer drop-off patterns and translate them into actionable content production guidelines.\n",
    "\n",
    "**Action:** I built an engagement funnel analysis tracking viewers through 5 milestones (1%, 25%, 50%, 75%, 100%). I discovered the biggest drop-off occurred at the **[biggest_drop_stage]** mark (**[biggest_drop_val]** percentage points). I segmented this by channel to identify which channels retained viewers best (**[best_retention_channel]**), then analyzed what those channels did differently.\n",
    "\n",
    "**Result:** This analysis led to three concrete recommendations: (1) stronger opening hooks in the first 15 seconds, (2) front-loading key messages before the **[biggest_drop_stage]** mark, and (3) establishing optimal duration guidelines based on engagement data. The overall completion rate benchmark was set at **[completion_rate]** with a target of improving by **[X]** percentage points.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Optimal Video Duration (Data-Driven Content Strategy)\n",
    "\n",
    "**Situation:** Content producers had no guidance on ideal video length. Videos ranged from under 1 minute to over 30 minutes with no data-driven rationale for duration choices.\n",
    "\n",
    "**Task:** Determine the optimal video duration that maximizes viewer engagement and completion.\n",
    "\n",
    "**Action:** I analyzed all videos across 7 duration buckets, measuring both completion rate and engagement score for each. I also compared what we were producing most (production volume) against what performed best (engagement).\n",
    "\n",
    "**Result:** Discovered a clear sweet spot at **[sweet_spot_bucket]** with **[sweet_spot_completion]** completion -- **[penalty_pp]** percentage points higher than **[worst_bucket]**. I also identified a production-vs-performance mismatch: we produced the most content at **[most_produced_bucket]** but the sweet spot was **[sweet_spot_bucket]**. This led to updated content guidelines and better resource allocation.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Channel Rationalization (Cost Optimization)\n",
    "\n",
    "**Situation:** The organization operated 11 Brightcove accounts, each with its own license, administration overhead, and content management workflow. There was no framework for evaluating which accounts delivered value.\n",
    "\n",
    "**Task:** Develop a data-driven framework to evaluate channel performance and identify consolidation opportunities.\n",
    "\n",
    "**Action:** I created a BCG-style matrix categorizing channels by reach (views) and engagement, classifying each as Stars (invest), Opportunities (promote), Cash Cows (maintain), or Reconsider (consolidate). I quantified the view share of each quadrant.\n",
    "\n",
    "**Result:** Identified **[reconsider_count]** channels in the \"Reconsider\" quadrant, representing only **[consolidation_pct]** of total views. Stars included **[stars]**, providing clear investment priorities. This framework supported the business case for account consolidation, potentially reducing licensing and administrative overhead.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Mobile Strategy (Trend Identification)\n",
    "\n",
    "**Situation:** Content was produced primarily for desktop consumption, but there was no data on how viewing habits were evolving across devices.\n",
    "\n",
    "**Task:** Quantify the device distribution and identify trends to inform platform investment decisions.\n",
    "\n",
    "**Action:** I tracked device-level viewing data (desktop, mobile, tablet) monthly, establishing a 30% mobile threshold as the trigger for a mobile-first content strategy. I correlated device type with engagement metrics to understand whether mobile viewers engaged differently.\n",
    "\n",
    "**Result:** Mobile viewing **[grew/declined]** from **[mobile_first_pct]** to **[mobile_last_pct]** (**[mobile_growth_pp]** pp). **[If above 30%: This exceeded the 30% threshold, justifying investment in mobile optimization: larger on-screen text, subtitles by default, and mobile-friendly thumbnails.]** **[If below 30%: While desktop-first remained appropriate, the growth trajectory informed planning for mobile investment within [X] months.]**\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Content Lifecycle Management (Operational Efficiency)\n",
    "\n",
    "**Situation:** The video library continued to grow with no systematic process for identifying or archiving stale content. This increased storage costs and made it harder for employees to find relevant, current content.\n",
    "\n",
    "**Task:** Develop a content freshness framework and identify archival candidates.\n",
    "\n",
    "**Action:** I tracked the `dt_last_viewed` field for every video and flagged content not viewed in 180+ days. I also analyzed content age distribution and compared recently created high performers vs underperformers to understand content vitality.\n",
    "\n",
    "**Result:** Identified **[stale_count]** stale videos representing **[stale_hours]** hours of stored content with **[stale_lifetime_views]** lifetime views. Recommended an archival process (excluding compliance materials) to reduce storage costs and improve content discoverability. Also established early-warning metrics for new content performance.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. Content Performance Intelligence (Stakeholder Value)\n",
    "\n",
    "**Situation:** Content producers had no feedback loop -- they published videos and never learned which resonated, which failed, or why. There was no equivalent of a \"greatest hits\" analysis or problem diagnosis.\n",
    "\n",
    "**Task:** Create a content performance framework that gives producers actionable intelligence, analogous to how Search Analytics identifies success stories, relevance problems, and content gaps.\n",
    "\n",
    "**Action:** I built three performance lenses: (1) \"Greatest Hits\" -- top videos by views and engagement for replication; (2) \"Problem Videos\" -- high impressions but low play rate (<30%), indicating thumbnail/title issues; (3) \"Hidden Gems\" -- high engagement (>70%) but low views (<200), indicating underpromotion opportunities.\n",
    "\n",
    "**Result:** Identified **[problem_video_count]** problem videos that were being shown but failing to convert (thumbnail/title issue), and **[hidden_gem_count]** hidden gems with strong engagement that deserved more promotion. This framework gave content producers data-driven feedback for the first time, shifting the organization from intuition-based to evidence-based content strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Close database connection\n",
    "# ---------------------------------------------------------------------------\n",
    "conn.close()\n",
    "print(\"Database connection closed.\")\n",
    "print(f\"Notebook completed: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}